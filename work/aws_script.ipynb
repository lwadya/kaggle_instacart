{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Script\n",
    "This notebook prototypes a script that runs on AWS and covers everything from SQL query to model training. Since my computer has neither the memory nor the processing power to handle the full Instacart dataset I'm offloading all of the data-intensive work to AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports\n",
    "* **var_to_pickle**: Writes the given variable to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Janky xgboost fix\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2 as pg\n",
    "import pandas.io.sql as pd_sql\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "from code.lw_pickle import var_to_pickle\n",
    "from code.lw_val_by_group import train_test_by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Connect to SQL Database on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_args = {\n",
    "    'host': '34.206.216.187',\n",
    "    'user': 'ubuntu',\n",
    "    'dbname': 'instacart',\n",
    "    'port': 5432\n",
    "}\n",
    "\n",
    "connection = pg.connect(**connection_args)\n",
    "csr = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Read SQL Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "206209 users, 1 to 206209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads from Train Orders Table\n",
    "cols = ['product_id',\n",
    "        'add_to_cart_order',\n",
    "        'user_id',\n",
    "        'order_number',\n",
    "        'order_dow',\n",
    "        'order_hour_of_day',\n",
    "        'days_since_prior_order']\n",
    "cols = ', '.join(cols)\n",
    "query = '''\n",
    "    SELECT orderstrain.order_id, %s\n",
    "    FROM orderstrain\n",
    "    INNER JOIN orders ON orderstrain.order_id = orders.order_id\n",
    "    WHERE user_id < 5000;\n",
    "''' % cols\n",
    "orders_train_df = pd_sql.read_sql(query, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads from Prior Orders Table\n",
    "cols = ['product_id',\n",
    "        'add_to_cart_order',\n",
    "        'user_id',\n",
    "        'order_number',\n",
    "        'order_dow',\n",
    "        'order_hour_of_day',\n",
    "        'days_since_prior_order']\n",
    "cols = ', '.join(cols)\n",
    "query = '''\n",
    "    SELECT ordersprior.order_id, %s\n",
    "    FROM ordersprior\n",
    "    INNER JOIN orders ON ordersprior.order_id = orders.order_id\n",
    "    WHERE user_id < 5000;\n",
    "''' % cols\n",
    "orders_prior_df = pd_sql.read_sql(query, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Merge Cart-level DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (orders_prior_df.groupby(['product_id', 'user_id'], as_index=False)\n",
    "                     .agg({'order_id':'nunique'})\n",
    "                     .rename(columns={'order_id':'count_in_user_orders'}))\n",
    "train_users = orders_train_df['user_id'].unique()\n",
    "df = df[df['user_id'].isin(train_users)]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "train_users = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target: Whether or Not Product is in Cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_carts_df = (orders_train_df.groupby('user_id')\n",
    "                                 .agg({'product_id':(lambda x: set(x))})\n",
    "                                 .rename(columns={'product_id':'cart_contents'}))\n",
    "\n",
    "df = df.merge(train_carts_df, on='user_id')\n",
    "df['in_cart'] = (df.apply(lambda row: row['product_id'] in row['cart_contents'], axis=1)\n",
    "                   .astype(int))\n",
    "df.drop('cart_contents', axis=1, inplace=True)\n",
    "train_carts_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: Product Order Rate by User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_per_user_df = (orders_prior_df.groupby(['user_id'])[['order_id']]\n",
    "                                    .nunique()\n",
    "                                    .rename(columns={'order_id':'total_user_orders'}))\n",
    "df = df.merge(prior_per_user_df, on='user_id')\n",
    "df['percent_in_user_orders'] = df['count_in_user_orders'] / df['total_user_orders']\n",
    "df.drop(['count_in_user_orders'], axis=1, inplace=True)\n",
    "prior_per_user_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: Product Overall Order Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_prior_df = (orders_prior_df.groupby(['product_id'], as_index=False)\n",
    "                                   .agg({'order_id':'nunique'})\n",
    "                                   .rename(columns={'order_id':'count_in_all_orders'})\n",
    "                                   .sort_values(by=['count_in_all_orders'], ascending=False)\n",
    "                                   .reset_index(drop=True))\n",
    "num_orders = orders_prior_df['order_id'].nunique()\n",
    "product_prior_df['percent_in_all_orders'] = (product_prior_df['count_in_all_orders'] /\n",
    "                                             num_orders)\n",
    "product_prior_df.drop('count_in_all_orders', axis=1, inplace=True)\n",
    "df = df.merge(product_prior_df, on='product_id')\n",
    "product_prior_df = num_orders = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: In Last Cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (orders_prior_df.sort_values(by='order_number')\n",
    "                       .groupby(['user_id'])['order_id']\n",
    "                       .last())\n",
    "last_contents_df = (orders_prior_df[orders_prior_df['order_id'].isin(mask)]\n",
    "                    .groupby(['user_id'])['product_id'].unique())\n",
    "last_contents_df = pd.DataFrame(last_contents_df)\n",
    "last_contents_df.rename(columns={'product_id':'last_cart_contents'}, inplace=True)\n",
    "\n",
    "df = df.merge(last_contents_df, how='left', on='user_id')\n",
    "df['in_last_cart'] =\\\n",
    "    (df.apply(lambda row: row['product_id'] in row['last_cart_contents'], axis=1)\n",
    "       .astype(int))\n",
    "df.drop('last_cart_contents', axis=1, inplace=True)\n",
    "last_contents_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: Days/Orders Between Orders, Times Product Appears in Last 5 Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "since_first_df = (orders_prior_df.groupby(['user_id', 'order_id'], as_index=False)\n",
    "                                 .agg({'order_number':'first',\n",
    "                                       'days_since_prior_order':'first'}))\n",
    "since_first_df = (since_first_df.drop('days_since_prior_order', axis=1)\n",
    "                                .merge(since_first_df.drop('order_id', axis=1),\n",
    "                                                           how='left',\n",
    "                                                           on='user_id'))\n",
    "\n",
    "mask = since_first_df['order_number_x'] <= since_first_df['order_number_y']\n",
    "since_newest_df = since_first_df[mask].drop(['user_id', 'order_number_y'], axis=1)\n",
    "since_newest_df = (since_newest_df.groupby(['order_id', 'order_number_x'],\n",
    "                                           as_index=False)['days_since_prior_order'].sum())\n",
    "since_newest_df.drop('order_number_x', axis=1, inplace=True)\n",
    "since_newest_df.rename(columns={'days_since_prior_order':'days_since_newest'}, inplace=True)\n",
    "\n",
    "mask = since_first_df['order_number_x'] >= since_first_df['order_number_y']\n",
    "since_first_df = since_first_df[mask].drop(['user_id', 'order_number_y'], axis=1)\n",
    "since_first_df = (since_first_df.groupby(['order_id', 'order_number_x'],\n",
    "                                         as_index=False)['days_since_prior_order'].sum())\n",
    "since_first_df.rename(columns={'days_since_prior_order':'days_since_first_order',\n",
    "                               'order_number_x':'order_number'}, inplace=True)\n",
    "\n",
    "newest_cart_df =\\\n",
    "    (orders_train_df.groupby(['user_id'])[['order_number', 'days_since_prior_order']]\n",
    "                    .first()\n",
    "                    .rename(columns={'order_number':'newest_order_number'}))\n",
    "\n",
    "orders_since_df = orders_prior_df[['user_id', 'order_id', 'product_id']]\n",
    "orders_since_df = orders_since_df.merge(since_first_df, how='left', on='order_id')\n",
    "orders_since_df = orders_since_df.merge(since_newest_df, how='left', on='order_id')\n",
    "orders_since_df = orders_since_df.merge(newest_cart_df, how='left', on='user_id')\n",
    "since_first_df = since_newest_df = newest_cart_df = None\n",
    "\n",
    "orders_since_df['days_since_newest'] += orders_since_df['days_since_prior_order']\n",
    "orders_since_df['orders_since_newest'] = (orders_since_df['newest_order_number'] -\n",
    "                                          orders_since_df['order_number'])\n",
    "\n",
    "mask = orders_since_df['order_number'] >= (orders_since_df['newest_order_number'] - 5)\n",
    "last_five_df =\\\n",
    "    (orders_since_df[mask].groupby(['user_id', 'product_id'], as_index=False)['order_id']\n",
    "                          .count()\n",
    "                          .rename(columns={'order_id':'in_last_five'}))\n",
    "orders_since_df = orders_since_df.merge(last_five_df,\n",
    "                                        how='left',\n",
    "                                        on=['user_id', 'product_id'])\n",
    "orders_since_df['in_last_five'] = orders_since_df['in_last_five'].fillna(0).astype(int)\n",
    "last_five_df = None\n",
    "\n",
    "orders_since_df.sort_values(by=['user_id', 'product_id', 'order_number'], inplace=True)\n",
    "orders_since_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "orders_since_df['last_order_number'] =\\\n",
    "    orders_since_df.groupby(['user_id', 'product_id'])['order_number'].shift(1)\n",
    "orders_since_df['last_days_since_first_order'] =\\\n",
    "    orders_since_df.groupby(['user_id', 'product_id'])['days_since_first_order'].shift(1)\n",
    "orders_since_df['mean_orders_between'] =\\\n",
    "    orders_since_df['order_number'] - orders_since_df['last_order_number']\n",
    "orders_since_df['mean_days_between'] =\\\n",
    "    orders_since_df['days_since_first_order'] - orders_since_df['last_days_since_first_order']\n",
    "\n",
    "(orders_since_df['mean_orders_between'].fillna(orders_since_df['orders_since_newest'],\n",
    "                                               inplace=True))\n",
    "(orders_since_df['mean_days_between'].fillna(orders_since_df['days_since_newest'],\n",
    "                                             inplace=True))\n",
    "\n",
    "orders_since_df = (orders_since_df.groupby(['user_id', 'product_id'], as_index=False)\n",
    "                                  .agg({'mean_orders_between':'mean',\n",
    "                                        'mean_days_between':'mean',\n",
    "                                        'orders_since_newest':'last',\n",
    "                                        'days_since_newest':'last',\n",
    "                                        'in_last_five':'last'}))\n",
    "orders_since_df.rename({'order_number':'lastest_order_number'}, inplace=True)\n",
    "\n",
    "df = df.merge(orders_since_df, how='left', on=['user_id', 'product_id'])\n",
    "orders_since_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: Likelihood a Product Gets Reordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_proba_df = (orders_prior_df.groupby('product_id', as_index=False)\n",
    "                                   .agg({'user_id':'nunique', 'order_id':'count'}))\n",
    "product_proba_df['product_reorder_proba'] = 1 - (product_proba_df['user_id'] /\n",
    "                                                 product_proba_df['order_id'])\n",
    "product_proba_df.drop(['user_id', 'order_id'], axis=1, inplace=True)\n",
    "df = df.merge(product_proba_df, how='left', on='product_id')\n",
    "product_proba_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: Likelihood a User Reorders Any Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proba_df = (orders_prior_df.groupby('user_id', as_index=False)\n",
    "                                .agg({'product_id':'nunique', 'order_id':'count'}))\n",
    "user_proba_df['user_reorder_proba'] = 1 - (user_proba_df['product_id'] /\n",
    "                                           user_proba_df['order_id'])\n",
    "user_proba_df.drop(['product_id', 'order_id'], axis=1, inplace=True)\n",
    "df = df.merge(user_proba_df, how='left', on='user_id')\n",
    "product_proba_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: Average Hour of Week, Order Size, and Add Order Percentile for Prior Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_of_week_df = (orders_prior_df.groupby('order_id')\n",
    "                                  .agg({'order_dow':'first',\n",
    "                                        'order_hour_of_day':'first',\n",
    "                                        'add_to_cart_order':'max'})\n",
    "                                  .rename(columns={'add_to_cart_order':'mean_cart_size'}))\n",
    "hour_of_week_df['mean_hour_of_week'] = (hour_of_week_df['order_dow'] * 24 +\n",
    "                                        hour_of_week_df['order_hour_of_day'])\n",
    "hour_of_week_df.drop(['order_dow', 'order_hour_of_day'], axis=1, inplace=True)\n",
    "\n",
    "percentile_df = (orders_prior_df[['user_id', 'order_id', 'product_id', 'add_to_cart_order']]\n",
    "                 .merge(hour_of_week_df, how='left', on='order_id'))\n",
    "percentile_df['mean_cart_percentile'] = (1 - (percentile_df['add_to_cart_order'] - 1) /\n",
    "                                         percentile_df['mean_cart_size'])\n",
    "percentile_df = (percentile_df.groupby(['user_id', 'product_id'])\n",
    "                              .agg({'mean_cart_size':'mean',\n",
    "                                    'mean_cart_percentile':'mean',\n",
    "                                    'mean_hour_of_week':'mean'}))\n",
    "\n",
    "df = df.merge(percentile_df, how='left', on=['user_id', 'product_id'])\n",
    "hour_of_week_df = percentile_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: Hour of Week and Number of Items in Newest Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_of_week_df = (orders_train_df.groupby('user_id')\n",
    "                                  .agg({'order_dow':'first',\n",
    "                                        'order_hour_of_day':'first',\n",
    "                                        'add_to_cart_order':'max'})\n",
    "                                  .rename(columns={'add_to_cart_order':'newest_cart_size'}))\n",
    "hour_of_week_df['newest_hour_of_week'] = (hour_of_week_df['order_dow'] * 24 +\n",
    "                                          hour_of_week_df['order_hour_of_day'])\n",
    "hour_of_week_df.drop(['order_dow', 'order_hour_of_day'], axis=1, inplace=True)\n",
    "\n",
    "df = df.merge(hour_of_week_df, how='left', on=['user_id'])\n",
    "hour_of_week_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: Absolute Difference in Cart Size, Hour of Week, Hour, and Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cart_size_difference'] = np.abs(df['mean_cart_size'] - df['newest_cart_size'])\n",
    "df['hour_of_week_difference'] = np.abs(df['mean_hour_of_week'] - df['newest_hour_of_week'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split Into Train, Validate, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_col = 'user_id'\n",
    "x_cols = ['percent_in_user_orders',\n",
    "          'percent_in_all_orders',\n",
    "          'in_last_cart',\n",
    "          'in_last_five',\n",
    "          'total_user_orders',\n",
    "          'mean_orders_between',\n",
    "          'mean_days_between',\n",
    "          'orders_since_newest',\n",
    "          'days_since_newest',\n",
    "          'product_reorder_proba',\n",
    "          'user_reorder_proba',\n",
    "          'mean_cart_size',\n",
    "          'mean_cart_percentile',\n",
    "          'mean_hour_of_week',\n",
    "          'newest_cart_size',\n",
    "          'newest_hour_of_week',\n",
    "          'cart_size_difference',\n",
    "          'hour_of_week_difference'\n",
    "         ]\n",
    "y_col = 'in_cart'\n",
    "\n",
    "train_df, test_df = train_test_by_group(df, group_col, test_size=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Generic Threshold Adjustment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_threshold(proba, threshold=.5):\n",
    "    return (proba[:, 1] >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=10, solver='lbfgs', multi_class='auto', max_iter=1000)\n",
    "lr.fit(train_df[x_cols], train_df[y_col])\n",
    "lr_proba = lr.predict_proba(test_df[x_cols])\n",
    "lr_pred = adjust_threshold(lr_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29828161066940245\n",
      "0.633442265795207\n",
      "0.1950687688695069\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(test_df[y_col], lr_pred))\n",
    "print(precision_score(test_df[y_col], lr_pred))\n",
    "print(recall_score(test_df[y_col], lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.450830801060322\n"
     ]
    }
   ],
   "source": [
    "lr_adj = adjust_threshold(lr_proba, .17)\n",
    "print(f1_score(test_df[y_col], lr_adj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = xgb.XGBClassifier(n_estimators=30000,\n",
    "                        max_depth=3,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        learning_rate=.5, \n",
    "                        subsample=.08,\n",
    "                        min_child_weight=.5,\n",
    "                        colsample_bytree=.8)\n",
    "gbm.fit(train_df[x_cols], train_df[y_col],\n",
    "        eval_set=[(train_df[x_cols], train_df[y_col])],\n",
    "        eval_metric='auc',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False)\n",
    "gbm_proba = gbm.predict_proba(test_df[x_cols])\n",
    "gbm_pred = adjust_threshold(gbm_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32601184600197436\n",
      "0.6167133520074697\n",
      "0.22156994297215699\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(test_df[y_col], gbm_pred))\n",
    "print(precision_score(test_df[y_col], gbm_pred))\n",
    "print(recall_score(test_df[y_col], gbm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45910309094157586\n"
     ]
    }
   ],
   "source": [
    "gbm_adj = adjust_threshold(gbm_proba, .21)\n",
    "print(f1_score(test_df[y_col], gbm_adj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Save Pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_to_pickle(test_df[x_cols], 'X_test.pk')\n",
    "var_to_pickle(test_df[y_col], 'y_test.pk')\n",
    "var_to_pickle(gbm, 'model_gbm.pk')\n",
    "var_to_pickle(lr, 'model_lr.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
